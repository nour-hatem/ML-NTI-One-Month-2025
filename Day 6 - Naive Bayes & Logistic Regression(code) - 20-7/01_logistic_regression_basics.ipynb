{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce89286",
   "metadata": {},
   "source": [
    "# Logistic Regression from Scratch\n",
    "Learn the fundamentals of logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecda27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a toy dataset\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_redundant=0,\n",
    "                           n_informative=2, random_state=1, n_clusters_per_class=1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Cost function\n",
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    h = sigmoid(X @ theta)\n",
    "    epsilon = 1e-5\n",
    "    return (-y.T @ np.log(h + epsilon) - (1 - y).T @ np.log(1 - h + epsilon)) / m\n",
    "\n",
    "# Gradient descent\n",
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    m = len(y)\n",
    "    for _ in range(iterations):\n",
    "        theta -= (alpha / m) * (X.T @ (sigmoid(X @ theta) - y))\n",
    "    return theta\n",
    "\n",
    "# Add intercept\n",
    "X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "theta = np.zeros((X_b.shape[1], 1))\n",
    "\n",
    "# Train model\n",
    "theta = gradient_descent(X_b, y, theta, alpha=0.1, iterations=1000)\n",
    "print(f\"Trained theta: {theta.ravel()}\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
